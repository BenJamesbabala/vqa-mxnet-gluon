{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Visual Question Answering in gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a notebook for implementing visual question answering in gluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as F\n",
    "import mxnet.contrib.ndarray as C\n",
    "import mxnet.gluon as gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "import bisect\n",
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import os\n",
    "from mxnet.test_utils import download\n",
    "import json\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The VQA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the VQA dataset, for each sample, there is one image and one question. The label is the answer for the question regarding the image. You can download the VQA1.0 dataset from <a href=\"http://www.visualqa.org/vqa_v1_download.html\">VQA</a> website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](img/pizza.png )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to preprocess the data:\n",
    "\n",
    "(1) Extract the samples from original json files. \n",
    "\n",
    "(2) Filter the samples giving top k answers(k can be 1000, 2000...). This will make the prediction easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually people use pretrained models to extract features from the image and question.\n",
    "\n",
    "__Image pretrained model__: \n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1409.1556\">**VGG**</a>: A key aspect of VGG was to use many convolutional blocks with relatively narrow kernels, followed by a max-pooling step and to repeat this block multiple times. \n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1512.03385\">**Resnet**</a>: It is a residual learning framework to ease the training of networks that are substantially deep. It reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.\n",
    "\n",
    "__Question pretrained model__: \n",
    "\n",
    "<a href=\"https://code.google.com/archive/p/word2vec/\">**Word2Vec**</a>: The word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The model contains 300-dimensional vectors for 3 million words and phrases. \n",
    "\n",
    "<a href=\"https://nlp.stanford.edu/projects/glove/\">**Glove**</a>: Similar to Word2Vec, it is a word embedding dataset. It contains 100/200/300-dimensional vectors for 2 million words.\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1506.06726\">**skipthought**</a>: This is an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. Different from the previous two model, this is a sentence based model.\n",
    "\n",
    "<a href=\"https://research.google.com/pubs/pub45610.html\">**GNMT encoder**</a>: We propose using the encoder of google neural machine translation system to extract the question features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The inputs of the data iterator are extracted image and question features. At each step, the data iterator will return a data batch list: question data batch and image data batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class VQAtrainIter(mx.io.DataIter):\n",
    "    def __init__(self, img, sentences, answer, batch_size, buckets=None, invalid_label=-1,\n",
    "                 text_name='text', img_name = 'image', label_name='softmax_label', dtype='float32', layout='NTC'):\n",
    "        super(VQAtrainIter, self).__init__()\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i in range(len(sentences)):\n",
    "            buck = bisect.bisect_left(buckets, len(sentences[i]))\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sentences[i])] = sentences[i]\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "        self.answer = answer\n",
    "        self.img = img\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.text_name = text_name\n",
    "        self.img_name = img_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nd_text = []\n",
    "        self.nd_img = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(text_name, (batch_size, self.default_bucket_key)),\n",
    "                                 (img_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(text_name, (self.default_bucket_key, batch_size)),\n",
    "                                 (img_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        self.nd_text = []\n",
    "        self.nd_img = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck.shape[0])\n",
    "            label = self.answer\n",
    "            self.nd_text.append(mx.ndarray.array(buck, dtype=self.dtype))\n",
    "            self.nd_img.append(mx.ndarray.array(self.img, dtype=self.dtype))\n",
    "            self.ndlabel.append(mx.ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            img = self.nd_img[i][j:j + self.batch_size].T\n",
    "            text = self.nd_text[i][j:j + self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "        else:\n",
    "            img = self.nd_img[i][j:j + self.batch_size]\n",
    "            text = self.nd_text[i][j:j + self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "        \n",
    "        data = [text, img]\n",
    "        return mx.io.DataBatch(data, [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.text_name, text.shape),(self.img_name, img.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we will use 1/10 of the VQA training data and 1/100 of the validation data to explain the model since the data is very large. We extract the image feature from ResNet-152, text feature from GNMT encoder. We have 21537 training samples and 1044 validation samples. Image feature is a 2048-dim vector. Question feature is a 1048-dim vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "demo = False\n",
    "dataset_files = {'train': ('train_question.npz','train_img.npz','train_ans.npz'),\n",
    "                 'validation': ('val_question.npz','val_img.npz','val_ans.npz'),\n",
    "                'test':('test_question_id.npz','test_question.npz','test_img_id.npz','test_img.npz','atoi.json','test_question_txt.json')}\n",
    "if demo:\n",
    "    train_q, train_i, train_a = dataset_files['validation']\n",
    "else:\n",
    "    train_q, train_i, train_a = dataset_files['train']\n",
    "val_q, val_i, val_a = dataset_files['validation']\n",
    "\n",
    "url_format = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/VQA-notebook/{}'\n",
    "if not os.path.exists(train_q):\n",
    "    logging.info('Downloading training dataset.')\n",
    "    download(url_format.format(train_q),overwrite=True)\n",
    "    download(url_format.format(train_i),overwrite=True)\n",
    "    download(url_format.format(train_a),overwrite=True)\n",
    "if not os.path.exists(val_q):\n",
    "    logging.info('Downloading validation dataset.')\n",
    "    download(url_format.format(val_q),overwrite=True)\n",
    "    download(url_format.format(val_i),overwrite=True)\n",
    "    download(url_format.format(val_a),overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "layout = 'NT'\n",
    "bucket = [1024]\n",
    "\n",
    "if demo:\n",
    "    train_question = np.load(\"val_question.npz\")['x']\n",
    "    val_question = np.load(\"val_question.npz\")['x']\n",
    "    train_ans = np.load(\"val_ans.npz\")['x']\n",
    "    val_ans = np.load(\"val_ans.npz\")['x']\n",
    "    train_img = np.load(\"val_img.npz\")['x']\n",
    "    val_img = np.load(\"val_img.npz\")['x']\n",
    "else:\n",
    "    train_question = np.load(\"train_question.npz\")['x']\n",
    "    val_question = np.load(\"val_question.npz\")['x']\n",
    "    train_ans = np.load(\"train_ans.npz\")['x']\n",
    "    val_ans = np.load(\"val_ans.npz\")['x']\n",
    "    train_img = np.load(\"train_img.npz\")['x']\n",
    "    val_img = np.load(\"val_img.npz\")['x']\n",
    "\n",
    "\n",
    "data_train  = VQAtrainIter(train_img, train_question, train_ans, batch_size, buckets = bucket,layout=layout)\n",
    "data_eva = VQAtrainIter(val_img, val_question, val_ans, batch_size, buckets = bucket,layout=layout) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define out model with gluon. gluon.Block is the basic building block of models. If any operator is not defined under gluon, you can use mxnet.ndarray operators to subsititude. \n",
    "\n",
    "In the __first model__, we will use multilayer perception(MLP) as the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Net(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.bn = nn.BatchNorm()\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.fc1 = nn.Dense(8192,activation=\"relu\")\n",
    "            self.fc2 = nn.Dense(1000)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.L2Normalization(x[0])\n",
    "        x2 = F.L2Normalization(x[1])\n",
    "        z = F.concat(x1,x2,dim=1)\n",
    "        z = self.fc1(z)\n",
    "        z = self.bn(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the __second model__, we use count sketch to estimate the outer product of the image and question features. \n",
    "\n",
    "This method was proposed in <a href=\"https://arxiv.org/abs/1606.01847\">Multimodal Compact Bilinear Pooling for VQA</a>. The key idea is:\n",
    "\n",
    "$\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\psi(x \\otimes q, h,s) = \\psi(x,h,s) \\star \\psi(y,h,s)$\n",
    "\n",
    "where $\\psi$ is the count sketch operator, $x,y$ are the inputs, $h, s$ are the hash tables, $\\otimes$ defines outer product and $\\star$ is the convolution operator. This can further be simplified by using FFT properties: convolution in time domain equares to elementwise product in frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu(2)\n",
    "compute_size  = batch_size\n",
    "out_dim = 10000\n",
    "gpus = len(ctx)\n",
    "class Net(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.bn = nn.BatchNorm()\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.fc1 = nn.Dense(8192,activation=\"relu\")\n",
    "            self.fc2 = nn.Dense(1000)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.L2Normalization(x[0])\n",
    "        x2 = F.L2Normalization(x[1])\n",
    "        text_ones = F.ones((batch_size/gpus, 2048),ctx = ctx)\n",
    "        img_ones = F.ones((batch_size/gpus, 1024),ctx = ctx)\n",
    "        text_data = F.Concat(x1, text_ones,dim = 1)\n",
    "        image_data = F.Concat(x2,img_ones,dim = 1)\n",
    "        \n",
    "        S1 = F.array(np.random.randint(0, 2, (batch_size/gpus,3072))*2-1,ctx = ctx)\n",
    "        H1 = F.array(np.random.randint(0, out_dim,(batch_size/gpus,3072)),ctx = ctx)\n",
    "        S2 = F.array(np.random.randint(0, 2, (batch_size/gpus,3072))*2-1,ctx = ctx)\n",
    "        H2 = F.array(np.random.randint(0, out_dim,(batch_size/gpus,3072)),ctx = ctx)\n",
    "        \n",
    "        cs1 = C.count_sketch( data = image_data, s=S1, h = H1 ,name='cs1',out_dim = out_dim) \n",
    "        cs2 = C.count_sketch( data = text_data, s=S2, h = H2 ,name='cs2',out_dim = out_dim) \n",
    "        fft1 = C.fft(data = cs1, name='fft1', compute_size = compute_size) \n",
    "        fft2 = C.fft(data = cs2, name='fft2', compute_size = compute_size) \n",
    "        c = fft1 * fft2\n",
    "        ifft1 = C.ifft(data = c, name='ifft1', compute_size = compute_size) \n",
    " \n",
    "        z = self.fc1(ifft1)\n",
    "        z = self.bn(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the __third model__, we introduce attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initialize the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# Initialize on CPU. Replace with `mx.gpu(0)`, or `[mx.gpu(0), mx.gpu(1)]`,\n",
    "# etc to use one or more GPUs.\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    \n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        with autograd.record():\n",
    "            data1 = batch.data[0].as_in_context(ctx)\n",
    "            data2 = batch.data[1].as_in_context(ctx)\n",
    "            data = [data1,data2]\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            #label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "        \n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "best_eva = 0\n",
    "for e in range(epochs):\n",
    "    data_train.reset()\n",
    "    for i, batch in enumerate(data_train):\n",
    "        data1 = batch.data[0].as_in_context(ctx)\n",
    "        data2 = batch.data[1].as_in_context(ctx)\n",
    "        data = [data1,data2]\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data[0].shape[0])\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = np.mean(cross_entropy.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(cross_entropy.asnumpy()[0])\n",
    "        #if i % 200 == 0:\n",
    "        #    print(\"Epoch %s, batch %s. Moving avg of loss: %s\" % (e, i, moving_loss))   \n",
    "    eva_accuracy = evaluate_accuracy(data_eva, net)\n",
    "    train_accuracy = evaluate_accuracy(data_train, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Eval_acc %s\" % (e, moving_loss, train_accuracy, eva_accuracy))\n",
    "    if eva_accuracy > best_eva:\n",
    "            best_eva = eva_accuracy\n",
    "            logging.info('Best validation acc found. Checkpointing...')\n",
    "            net.save_params('vqa-mlp-%d.params'%(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can try it on test data. Here we have 10 test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = True\n",
    "if test:\n",
    "    test_q_id, test_q, test_i_id, test_i, atoi,text = dataset_files['test']\n",
    "\n",
    "if test and not os.path.exists(test_q):     \n",
    "    logging.info('Downloading test dataset.')\n",
    "    download(url_format.format(test_q_id),overwrite=True)\n",
    "    download(url_format.format(test_q),overwrite=True)\n",
    "    download(url_format.format(test_i_id),overwrite=True)\n",
    "    download(url_format.format(test_i),overwrite=True)\n",
    "    download(url_format.format(atoi),overwrite=True)\n",
    "download(url_format.format(text),overwrite=True)\n",
    "\n",
    "if test:\n",
    "    test_question = np.load(\"test_question.npz\")['x']\n",
    "    test_img = np.load(\"test_img.npz\")['x']\n",
    "    test_question_id = np.load(\"test_question_id.npz\")['x']\n",
    "    test_img_id = np.load(\"test_img_id.npz\")['x']\n",
    "    #atoi = np.load(\"atoi.json\")['x']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We pass the test data iterator to the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_test = VQAtrainIter(test_img, test_question, np.zeros((test_img.shape[0],1)), 10, buckets = bucket,layout=layout)\n",
    "for i, batch in enumerate(data_test):\n",
    "    with autograd.record():\n",
    "        data1 = batch.data[0].as_in_context(ctx)\n",
    "        data2 = batch.data[1].as_in_context(ctx)\n",
    "        data = [data1,data2]\n",
    "        #label = batch.label[0].as_in_context(ctx)\n",
    "        #label_one_hot = nd.one_hot(label, 10)\n",
    "        output = net(data)\n",
    "output = np.argmax(output.asnumpy(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(10)\n",
    "print(idx)\n",
    "question = json.load(open(text))\n",
    "print(\"Question:\", question[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_name = 'COCO_test2015_' + str(int(test_img_id[idx])).zfill(12)+'.jpg'\n",
    "if not os.path.exists(image_name):\n",
    "    logging.info('Downloading training dataset.')\n",
    "    download(url_format.format('test_images/'+image_name),overwrite=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename=image_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = json.load(open('atoi.json'))\n",
    "ans = dataset['ix_to_ans'][str(output[idx]+1)]\n",
    "print(\"Answer:\", ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
